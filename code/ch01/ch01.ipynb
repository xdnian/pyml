{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Copyright (c) 2015, 2016\n",
    "[Sebastian Raschka](http://sebastianraschka.com/)\n",
    "[Li-Yi Wei](http://www.liyiwei.org/)\n",
    "\n",
    "https://github.com/1iyiwei/pyml\n",
    "<br>\n",
    "[MIT License](https://github.com/1iyiwei/pyml/blob/master/LICENSE.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "* What is machine learning\n",
    "* Why machine learning\n",
    "* Types of machine learning\n",
    "* Machine learning pipeline\n",
    "* Relations to other fields\n",
    "* Historical perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is machine learning?\n",
    "The construction and study of algorithms/programs that can <b> learn from data </b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./images/ml-overview.jpg?raw=true)<!-- \n",
    "![](../images/ml-overview.jpg?raw=true)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Traditional programming\n",
    "\n",
    "Steps: formulate problem $\\rightarrow$ design algorithm $\\rightarrow$ write program $\\rightarrow$ test\n",
    "\n",
    "The program remains invariant with different input data, unless the programmer manually changes it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example: minimum finding\n",
    "Problem\n",
    "* Given a sequence of numbers, find the smallest one\n",
    "\n",
    "Algorithm\n",
    "* Record the currently minimum, initialized to $\\infty$\n",
    "* Loop through the numbers one by one\n",
    " * If this number $<$ minimum, minimum $\\leftarrow$ this number\n",
    " \n",
    "Analysis\n",
    "* The time complexity of the above algorithm is $O(N)$ where $N$ is the sequence size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.2\n"
     ]
    }
   ],
   "source": [
    "# a simple python program to find minimum numbers\n",
    "# note that it remains the same regardless of the input data\n",
    "\n",
    "import math\n",
    "\n",
    "# function\n",
    "def findmin(numbers):\n",
    "    answer = math.inf\n",
    "    for value in numbers:\n",
    "        if value < answer:\n",
    "            answer = value\n",
    "    return answer\n",
    "\n",
    "# main\n",
    "test = [3.14, 2.2, 8, -9.2, 100000, 0]\n",
    "\n",
    "print(findmin(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example: sorting\n",
    "\n",
    "Problem\n",
    "* Given a sequence of numbers, order them from small to large\n",
    "\n",
    "Algorithm\n",
    "* Pick one number (randomly) as anchor\n",
    "* Go through each other number\n",
    " * If this number $\\leq$ anchor, append it to the left sequence\n",
    " * Otherwise, append it to the right sequence\n",
    "* Apply the same method recursively to the left and right sequences\n",
    "* Concatenate left sequence, anchor, right sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# code is left as an exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Other examples\n",
    "Think about the programs you have written in the past; how many of them can learn from data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning\n",
    "\n",
    "The program can learn from data and change structure/behavior\n",
    "\n",
    "The programmer still writes (part of) the program, but it is not fixed\n",
    "* Models with parameters that can change with input data, like brains\n",
    "* Programmer selects and initializes the model; the model parameters change with data (training via optimization)\n",
    "* The trained model deals with future situations\n",
    "\n",
    "[<img src=\"https://www.wired.com/wp-content/uploads/2016/03/GW20160133774-1024x768.jpg\">](http://www.wired.com/2016/03/final-game-alphago-lee-sedol-big-deal-humanity/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why learning from data\n",
    "\n",
    "Some algorithms/programs are hard/impossible to design/code manually/explicitly\n",
    "\n",
    "The algorithms/programs might need to deal with unforseeable situations (generalization)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example: handwriting digit recognition\n",
    "\n",
    "<a href=\"https://github.com/cazala/mnist/blob/master/README.md\"><img src=\"https://camo.githubusercontent.com/d440ac2eee1cb3ea33340a2c5f6f15a0878e9275/687474703a2f2f692e7974696d672e636f6d2f76692f3051493378675875422d512f687164656661756c742e6a7067\"></a>\n",
    "\n",
    "Problem\n",
    "* Input: a digit represented by a $28 \\times 28$ image (MNIST)\n",
    "* Output: one of the digits in [0 .. 9]\n",
    "\n",
    "Traditional programming?\n",
    "* Give it a try :-)\n",
    "\n",
    "Machine learning\n",
    "* Collect data - pairs of images and labels\n",
    "* Select and initialize a model; train the model (parameters) with the data\n",
    "* The model, if properly trained, can recognize handwritings not in the original dataset for training\n",
    "\n",
    "Sometimes it is much easier to say what (example data) instead of how (algorithm)\n",
    "* [Soon We Won’t Program Computers. We’ll Train Them Like Dogs](http://www.wired.com/2016/05/the-end-of-code/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Other applications\n",
    "\n",
    "* Self-driving cars\n",
    "* Language translation\n",
    "* Speech analysis & synthesis\n",
    "* Spam filtering\n",
    "* Recommendation systems\n",
    "* Fraud detection\n",
    "* Market prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Types of machine learning\n",
    "\n",
    "<img src=\"./images/01_01.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised learning\n",
    "\n",
    "Given examples of inputs and corresponding desired outputs, predict outputs for future inputs\n",
    "* classification, regression, time series prediction\n",
    "\n",
    "<img src=\"./images/01_02.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification vs. regression\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classification: discrete output\n",
    "Class labels: spam or not spam, positive or negative for disease, types of animals, etc.\n",
    "<img src=\"./images/01_03.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifying flowers based on various features\n",
    "<img src=\"./images/01_08.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Regression: continous output\n",
    "function fitting\n",
    "\n",
    "<img src=\"./images/01_04.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised learning\n",
    "\n",
    "Given only inputs (without desired output examples), automatically discover representations, features, structure, etc. \n",
    "* clustering, outlier detection, density estimation, dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Clustering\n",
    "\n",
    "Put data samples into different groups\n",
    "\n",
    "<img src=\"./images/01_06.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "Project data from a higher dimensional space into a lower dimensional space\n",
    "* compression, visualization\n",
    "\n",
    "<img src=\"./images/01_07.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement learning\n",
    "\n",
    "Given sequences of actions of an agent and feedbacks from an environment, learn to select action sequences in a way that maximises the expected reward\n",
    "\n",
    "<img src=\"./images/01_05.png\" width=50%>\n",
    "\n",
    "* playing games\n",
    " * state: game board configuration\n",
    " * reward: expected winning chance\n",
    " \n",
    "* self driving cars\n",
    " * state: position, direction, speed, sensors, etc.\n",
    " * reward: not hitting anything, remaining time for destination, fuel consumption, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary of types\n",
    "\n",
    "Types of learning\n",
    "\n",
    "* Supervised learning\n",
    " * Given sample inputs and outputs, learn how to predict outputs for future inputs\n",
    "\n",
    "* Unsupervised learning\n",
    " * Given only sample inputs, learn to transform or organize them in some way\n",
    "\n",
    "* Reinforcement learning\n",
    " * No direct inputs or outputs, learn best behavior from environment feedbacks (states, rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    "Types of data\n",
    "* Discrete/continuous $\\times$ input/output\n",
    " * Applies to different types of learning above\n",
    "\n",
    "* Discrete output\n",
    " * Classification for supervised learning\n",
    " * Clustering for unsupervised learning\n",
    " \n",
    "* Continuous output\n",
    " * Regression for supervised learning\n",
    " * Dimensionality reduction for unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Representation\n",
    "\n",
    "## Model\n",
    "We can represent the model as a function $f$, with a set of parameters $\\Theta$.\n",
    "<br>\n",
    "Given a set of inputs $\\mathbf{X}$, the model computes outcomes $\\mathbf{Y}$.\n",
    "$$\\mathbf{Y} = f(\\mathbf{X}, \\Theta)$$\n",
    "\n",
    "For example, in digit recognition, $\\mathbf{X}$ and $\\mathbf{Y}$ are the digit images and digit labels (0 to 9), respectively.\n",
    "\n",
    "The parameters $\\Theta$ consist of those optimized automatically and those manually picked by humans.\n",
    "The latter are called <b>hyper</b>-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Loss\n",
    "Every machine learning task as a goal, which can be formalized as a loss function:\n",
    "$$L(\\mathbf{X}, \\mathbf{T}, \\mathbf{Y})$$\n",
    ", where $\\mathbf{T}$ is some form of target or auxiliary information, such as:\n",
    "* labels for supervised classification\n",
    "* number of clusters for unsupervised clustering\n",
    "* environment for reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Regularization\n",
    "In addition to the objective, we often care about the simplicity of the model, for better efficiency and generalization (avoiding over-fitting).\n",
    "The complexity of the model can be measured by another penalty function:\n",
    "$$P(\\Theta)$$\n",
    "Some common penalty functions include number and/or magnitude of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Objective\n",
    "We can sum up both the loss and regularization terms as the total objective:\n",
    "$$\\Phi(\\mathbf{X}, \\mathbf{T}, \\Theta) = L\\left(\\mathbf{X}, \\mathbf{T}, \\mathbf{Y}=f(\\mathbf{X}, \\Theta)\\right) + P(\\Theta)$$\n",
    "\n",
    "During training, the goal is to optimize the parameters $\\Theta$ with respect to the given training data $\\mathbf{X}$ and $\\mathbf{T}$:\n",
    "$$argmin_\\Theta \\; \\Phi(\\mathbf{X}, \\mathbf{T}, \\Theta)$$\n",
    "And hope the trained model with generalize well to future data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: curve fitting\n",
    "\n",
    "<img src=\"./images/01_04.png\" width=50%>\n",
    "\n",
    "Given a set of data points $\\left(\\mathbf{X}, \\mathbf{Y}\\right)$, fit a model curve to describe their relationship.\n",
    "\n",
    "This is actually a regression problem, but we have all seen this in prior math/coding classes to serve as a good example for machine learning.\n",
    "\n",
    "Recall $\\mathbf{Y} = f(\\mathbf{X}, \\Theta)$ is our model.\n",
    "\n",
    "For 2D linear curve fitting, the model is a straight line:\n",
    "$y = w_1 x + w_0$, so the parameters $\\Theta = \\{w_0, w_1\\}$.\n",
    "\n",
    "The loss function is $L\\left(\\mathbf{X}, \\mathbf{T}, \\mathbf{Y}\\right) = \\sum_i \\left( T^{(i)} - Y^{(i)}\\right)^2 = \\sum_i \\left( T^{(i)} - w_1 X^{(i)} - w_0 \\right)^2$.\n",
    "<br>\n",
    "($\\mathbf{X}$ is a matrix/tensor, and each data sample is a row. We denote the ith sample/row as $\\mathbf{X}^{(i)}$.)\n",
    "\n",
    "For this simple example we don't care about regularization, thus $P(\\Theta) = 0$.\n",
    "\n",
    "The goal is to optimize $\\Theta = \\{w_0, w_1 \\}$ with given $\\left(\\mathbf{X}, \\mathbf{Y}\\right)$ to minimize $L$.\n",
    "For simple cases like this, we can directly optimize via calculus:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_0} & = 0 \\\\\n",
    "\\frac{\\partial L}{\\partial w_1} & = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The math and coding will be left as an exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Steps for building a machine learning system\n",
    "\n",
    "We will talk about individual steps for the rest of this course.\n",
    "\n",
    "<img src=\"./images/01_09.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning and other related fields\n",
    "\n",
    "Data mining\n",
    "* discovering (i.e. mining) useful information from large data sets\n",
    "\n",
    "Pattern recognition\n",
    "* originated from engineering, more on hand-crafted algorithms\n",
    "* machine learning originated from computer science\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Artificial intelligence\n",
    "* machine learning is a subset\n",
    "* traditional AI can be rule-based, deterministic\n",
    "* machine learnign tends to be data-driven, probabilistic\n",
    " * statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Cognitive science\n",
    "* reverse engineers the brain\n",
    "* machine learning focuses on the forward process\n",
    " * understanding biology can help, but not the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep learning\n",
    "\n",
    "Deep learning $\\subset$ neural networks $\\subset$ machine learning\n",
    "\n",
    "Algorithms existed since the 80's, but lacked sufficient computing power\n",
    "\n",
    "Moore law enabled simple algorithms to process deep architecture and large data\n",
    "\n",
    "<a href=\"http://www.macleans.ca/society/science/the-meaning-of-alphago-the-ai-program-that-beat-a-go-champ/\">Geoffrey Hinton, the godfather of ‘deep learning’—which helped Google’s AlphaGo beat a grandmaster—on the past, present and future of AI</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reading\n",
    "* PML - Chapter 1\n",
    "* IML - Chapter 1\n",
    "\n",
    "# Coding\n",
    "* Install anaconda, git\n",
    "* Review/learn python, ipynb\n",
    "* See [README.md](https://github.com/1iyiwei/pyml/blob/master/code/ch01/README.md) for quick instructions\n",
    "\n",
    "# Assignment\n",
    "* [ex01](http://nbviewer.jupyter.org/github/1iyiwei/pyml/blob/master/code/ch01/ex01.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h1>Fin</h1>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
