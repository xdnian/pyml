{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 3 - basic classifiers\n",
    "\n",
    "Math practice and coding application for main classifiers introduced in Chapter 3 of the Python machine learning book. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Weighting\n",
    "\n",
    "Note that this assignment is more difficult than the previous ones, and thus has a higher weighting 3 and longer duration (3 weeks). Each one of the previous two assignments has a weighting 1.\n",
    "\n",
    "Specifically, the first 3 assignments contribute to your continuous assessment as follows:\n",
    "\n",
    "Assignment weights: $w_1 = 1, w_2 = 1, w_3 = 3$\n",
    "\n",
    "Assignment grades: $g_1, g_2, g_3$\n",
    "\n",
    "Weighted average: $\\frac{1}{\\sum_i w_i} \\times \\sum_i \\left(w_i \\times g_i \\right)$\n",
    "\n",
    "Future assignments will be added analogously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RBF kernel (20 points)\n",
    "\n",
    "Show that a Gaussian RBF kernel can be expressed as a dot product:\n",
    "$$\n",
    "K(\\mathbf{x}, \\mathbf{y}) \n",
    "= e^\\frac{-|\\mathbf{x} - \\mathbf{y}|^2}{2} \n",
    "= \\phi(\\mathbf{x})^T \\phi(\\mathbf{y})\n",
    "$$\n",
    "by spelling out the mapping function $\\phi$.\n",
    "\n",
    "For simplicity\n",
    "* you can assume both $\\mathbf{x}$ and $\\mathbf{y}$ are 2D vectors\n",
    "$\n",
    "x =\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{pmatrix}\n",
    ", \\;\n",
    "y =\n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2\n",
    "\\end{pmatrix}\n",
    "$\n",
    "* we use a scalar unit variance here\n",
    "\n",
    "even though the proof can be extended for vectors $\\mathbf{x}$ $\\mathbf{y}$ and general covariance matrices.\n",
    "\n",
    "Hint: use Taylor series expansion of the exponential function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "According to Taylor series,\n",
    "$$\n",
    "K(\\mathbf{x}, \\mathbf{y}) \n",
    "= e^\\frac{-|\\mathbf{x} - \\mathbf{y}|^2}{2} \n",
    "= e^\\frac{-\\left \\| \\mathbf{x} \\right \\|^2}{2}\\cdot e^\\frac{-\\left \\| \\mathbf{y} \\right \\|^2}{2}\\cdot e^{\\mathbf{x}^{T} \\mathbf{y}}\n",
    "= e^\\frac{-\\left \\| \\mathbf{x} \\right \\|^2}{2}\\cdot e^\\frac{-\\left \\| \\mathbf{y} \\right \\|^2}{2}\n",
    "\\cdot \\sum_{n=0}^{\\infty}\\frac{(\\mathbf{x}^T \\mathbf{y})^n}{n!}\n",
    "$$\n",
    "\n",
    "Let \n",
    "$$K'(\\mathbf{x}, \\mathbf{y}) = \\sum_{n=0}^{\\infty}\\frac{(\\mathbf{x}^T \\mathbf{y})^n}{n!}=1+(x_1y_1+x_2y_2)+\\frac{(x_1y_1+x_2y_2)^2}{2}+\\cdots,\n",
    "$$ \n",
    "which is a polynomial kernel of degree $n$.\n",
    "\n",
    "Thus, $K'(\\mathbf{x}, \\mathbf{y})$ corresponds to kernel function \n",
    "$\\phi^*(x) = [1, x_1, x_2, \\frac{{x_1}^{2}}{\\sqrt{2}}, x_1x_2, \\frac{{x_2}^{2}}{\\sqrt{2}}, ...]$, and\n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{x}) = e^{-\\frac{||\\mathbf{x}||^2}{2}} \\cdot \\phi^*(\\mathbf{x})= e^{-\\frac{||\\mathbf{x}||^2}{2}} \\cdot [1, x_1, x_2, \\frac{{x_1}^{2}}{\\sqrt{2}}, x_1x_2, \\frac{{x_2}^{2}}{\\sqrt{2}}, ...]\n",
    "$$\n",
    "\n",
    "To be specified, the $(\\frac{n(n+1)}{2}+i+1)^{th}$ item in $\\phi^*(\\mathbf{x})$ is,\n",
    "\n",
    "$$\n",
    "\\phi_{\\frac{n(n+1)}{2}+i+1}^*(\\mathbf{x}) = \\frac{x_1^i x_2^{n-i}}{\\sqrt{i!(n-i)!}},\\quad \\forall n=0,1,2,\\ldots \\,\\,\\, \\textrm{and} \\quad i=0,1,2,\\ldots,n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kernel SVM complexity (10 points)\n",
    "\n",
    "How would the complexity (in terms of number of parameters) of a trained kernel SVM change with the amount of training data, and why?\n",
    "Note that the answer may depend on the specific kernel used as well as the amount of training data.\n",
    "Consider specifically the following types of kernels $K(\\mathbf{x}, \\mathbf{y})$.\n",
    "* linear:\n",
    "$$\n",
    "K\\left(\\mathbf{x}, \\mathbf{y}\\right) = \\mathbf{x}^T \\mathbf{y}\n",
    "$$\n",
    "* polynomial with degree $q$:\n",
    "$$\n",
    "K\\left(\\mathbf{x}, \\mathbf{y}\\right) =\n",
    "(\\mathbf{x}^T\\mathbf{y} + 1)^q\n",
    "$$\n",
    "* RBF with distance function $D$:\n",
    "$$\n",
    "K\\left(\\mathbf{x}, \\mathbf{y} \\right) = e^{-\\frac{D\\left(\\mathbf{x}, \\mathbf{y} \\right)}{2s^2}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $n$ is the number of parameters.\n",
    "\n",
    "For linear kernel $K\\left(\\mathbf{x}, \\mathbf{y}\\right) = \\mathbf{x}^T \\mathbf{y}$, the mapping function is $\\phi(x)=[x_1, x_2, x_3, ..., x_n]^T$. Thus, the complexity is $\\mathbf{O}(n)$. The complexity will linearly increase as the increasement of amount of training data.\n",
    "\n",
    "For polynomial kernel, $K(\\mathbf{x},\\mathbf{y})=\\left(\\mathbf{x}^{T}\\mathbf{y}+1\\right)^{q}=\\left(1+x_1 y_1+x_2 y_2+\\ldots+x_n y_n\\right)^{q}=\\phi(\\mathbf{x})^{T}\\phi(\\mathbf{y})$, where $\\phi(\\mathbf{x})$ has $\\binom{n+q}{n}$ elements. Thus, the complexity is $\\mathbf{O\\left( n^q \\right)}$. The complexity will increase to the power of $q$ as the increasement of amount of training data.\n",
    "\n",
    "For RBF kernel $K\\left(\\mathbf{x}, \\mathbf{y} \\right) = e^{-\\frac{D\\left(\\mathbf{x}, \\mathbf{y} \\right)}{2s^2}}$, as shown in Q1, the $K\\left(\\mathbf{x}, \\mathbf{y} \\right)$ can be written in the form of $\\phi(\\mathbf{x})^T \\cdot \\phi(\\mathbf{y})$, in which the $\\phi(\\mathbf{x})$ is of infinite dimension. Thus, the complexity is infinite and remain the same when the number of parameter increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian density Bayes (30 points)\n",
    "\n",
    "$$\n",
    "p\\left(\\Theta | \\mathbf{X}\\right)\n",
    "= \n",
    "\\frac{p\\left(\\mathbf{X} | \\Theta\\right) p\\left(\\Theta\\right)}{p\\left(\\mathbf{X}\\right)}\n",
    "$$\n",
    "\n",
    "Assume both the likelihood and prior have Gaussian distributions:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{X} | \\Theta)\n",
    "&=\n",
    "\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right)\n",
    "\\\\\n",
    "p(\\Theta)\n",
    "&=\n",
    "\\frac{1}{\\sqrt{2\\pi}\\sigma_0} \\exp\\left( -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Derive $\\Theta$ from the dataset $\\mathbf{X}$ via the following methods:\n",
    "\n",
    "### ML (maximum likelihood) estimation \n",
    "$$\n",
    "\\Theta_{ML} = argmax_{\\Theta} p(\\mathbf{X} | \\Theta)\n",
    "$$\n",
    "\n",
    "### MAP estimation\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Theta_{MAP} \n",
    "&= \n",
    "argmax_{\\Theta} p(\\Theta | \\mathbf{X})\n",
    "\\\\\n",
    "&=\n",
    "argmax_{\\Theta} p(\\mathbf{X} | \\Theta) p(\\Theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Bayes estimation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Theta_{Bayes} \n",
    "&= \n",
    "E(\\Theta | \\mathbf{X})\n",
    "\\\\\n",
    "&= \n",
    "\\int \\Theta p(\\Theta | \\mathbf{X}) d\\Theta\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ML (maximum likelihood) estimation\n",
    "$$\n",
    "\\begin{align}\n",
    "arg\\max_{\\Theta} p(\\mathbf{X} | \\Theta)\n",
    "&\\Leftrightarrow arg\\max_{\\Theta} \\left(exp\\left(-\\frac{\\sum_{t=1}^{N}(x^{(t)}-\\Theta)^2)}{2\\sigma^2}\\right)\\right) \n",
    "\\\\\n",
    "&\\Leftrightarrow arg\\min_{\\Theta} \\sum_{t=1}^N \\left(\\mathbf{x}^{(t)} - \\Theta \\right)^2 \n",
    "\\\\\n",
    "&\\Rightarrow \\frac{\\partial }{\\partial \\Theta}\\sum_{t=1}^N \\left(\\mathbf{x}^{(t)} - \\Theta \\right)^2 = 0 \n",
    "\\\\\n",
    "&\\therefore \\Theta_{ML} = \\bar{x}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### MAP estimation\n",
    "$$\n",
    "\\begin{align}\n",
    "arg\\max_{\\Theta} p(\\Theta | \\mathbf{X}) \n",
    "&\\Leftrightarrow arg\\max_{\\Theta}\\left(\\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2} -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\\right) \n",
    "\\\\\n",
    "&\\Leftrightarrow arg\\min_{\\Theta}\\left( \\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2} + \\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right) \n",
    "\\\\\n",
    "&\\Rightarrow \\frac{\\partial }{\\partial \\Theta}\\left( \\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2} + \\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right) = 0 \n",
    "\\\\\n",
    "&\\therefore \\Theta_{MAP} = \\frac{\\frac{N\\bar{x}}{\\sigma^2}+\\frac{\\mu_0}{\\sigma_0^2}}{\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Bayes estimation\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\Theta|\\mathbf{X})\n",
    "&\\propto p(\\mathbf{X}|\\Theta)p(\\Theta)\n",
    "\\\\\n",
    "&\\propto \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2} -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\n",
    "\\\\\n",
    "&= \\exp\\left(-\\frac{\\sum_{t=1}^{N}{\\left(\\mathbf{x}^{(t)}-\\bar{\\mathbf{X}}\\right)}^{2}+N\\left(\\bar{\\mathbf{X}}-\\Theta\\right)^{2}}{2\\sigma^2}-\\frac{(\\Theta-\\mu_0)^{2}}{2{\\sigma_0}^{2}}\\right)\n",
    "\\\\\n",
    "&\\propto \\exp\\left(-\\frac{N\\left(\\bar{\\mathbf{X}}-\\Theta\\right)^{2}}{2\\sigma^2}-\\frac{(\\Theta-\\mu_0)^{2}}{2{\\sigma_0}^{2}}\\right)\n",
    "\\\\\n",
    "&\\propto \\exp\\left(-\\left(\\frac{N}{2\\sigma^2}+\\frac{1}{2{\\sigma_0}^{2}}\\right)\\Theta^2+\\left(\\frac{N\\bar{\\mathbf{X}}}{\\sigma^2}+\\frac{\\mu_0}{{\\sigma_0}^{2}}\\right)\\Theta\\right)\n",
    "\\\\\n",
    "&\\propto\n",
    "\\exp\\left(-\\frac{(\\Theta-\\mu_0')^2}{2{\\sigma'}^2}\\right)\n",
    "\\\\\n",
    "&\n",
    "where \\space \\mu_0' = \\frac{\\frac{N\\bar{x}}{\\sigma^2}+\\frac{\\mu_0}{\\sigma_0^2}}{\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}},\\space {\\sigma'}^2=\\frac{1}{\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\therefore \\Theta|\\mathbf{X}\\,\\,\\textrm{~}\\,\\, N\\left(\\mu_0',\\sigma'\\right) \\space , and \\quad\n",
    "\\Theta_{Bayes} = E(\\Theta|\\mathbf{X}) = \\mu_0' = \\frac{\\frac{N\\bar{x}}{\\sigma^2}+\\frac{\\mu_0}{\\sigma_0^2}}{\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hand-written digit classification (40 points)\n",
    "\n",
    "In the textbook sample code we applied different scikit-learn classifers for the Iris data set.\n",
    "\n",
    "In this exercise, we will apply the same set of classifiers over a different data set: hand-written digits.\n",
    "Please write down the code for different classifiers, choose their hyper-parameters, and compare their performance via the accuracy score as in the Iris dataset.\n",
    "Which classifier(s) perform(s) the best and worst, and why?\n",
    "\n",
    "The classifiers include:\n",
    "* perceptron\n",
    "* logistic regression\n",
    "* SVM\n",
    "* decision tree\n",
    "* random forest\n",
    "* KNN\n",
    "* naive Bayes\n",
    "\n",
    "The dataset is available as part of scikit learn, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last updated: 2016-10-20 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 4.2.0\n",
      "\n",
      "numpy 1.11.1\n",
      "pandas 0.18.1\n",
      "matplotlib 1.5.3\n",
      "scipy 0.18.0\n",
      "sklearn 0.17.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a '' -u -d -v -p numpy,pandas,matplotlib,scipy,sklearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Added version check for recent scikit-learn 0.18 checks\n",
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.data # training data\n",
    "y = digits.target # training label\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEZCAYAAADCJLEQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfx5v3tA/HpL1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2623935e5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "num_rows = 4\n",
    "num_cols = 5\n",
    "\n",
    "fig, ax = plt.subplots(nrows=num_rows, ncols=num_cols, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for index in range(num_rows*num_cols):\n",
    "    img = digits.images[index]\n",
    "    label = digits.target[index]\n",
    "    ax[index].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[index].set_title('digit ' + str(label))\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Date Preprocessing\n",
    "Hint: How you divide training and test data set? And apply other techinques we have learned if needed.\n",
    "You could take a look at the Iris data set case in the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 1257, test: 540\n"
     ]
    }
   ],
   "source": [
    "#Your code comes here\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "if Version(sklearn_version) < '0.18':\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "num_training = y_train.shape[0]\n",
    "num_test = y_test.shape[0]\n",
    "print('training: ' + str(num_training) + ', test: ' + str(num_test))\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #1 Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Perceptron] Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppn = Perceptron(n_iter = 40, eta0 = 0.1, random_state = 0)\n",
    "ppn.fit(X_train_std, y_train)\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "print('[Perceptron] Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Logistic Regression] Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(C = 1000.0, random_state = 0)\n",
    "lr.fit(X_train_std, y_train)\n",
    "y_pred = lr.predict(X_test_std)\n",
    "print('[Logistic Regression] Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #3 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear SVM] Accuracy: 0.97\n",
      "[RBF SVM] Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Linear SVM\n",
    "svm0 = SVC(kernel='linear', C = 1.0, random_state = 0)\n",
    "svm0.fit(X_train_std, y_train)\n",
    "y_pred0 = svm0.predict(X_test_std)\n",
    "\n",
    "#RBF SVM\n",
    "svm1 = SVC(kernel='rbf', random_state = 0, gamma = 0.1, C = 1.0)\n",
    "svm1.fit(X_train_std, y_train)\n",
    "y_pred1 = svm1.predict(X_test_std)\n",
    "\n",
    "print('[Linear SVM] Accuracy: %.2f' % accuracy_score(y_test, y_pred0))\n",
    "print('[RBF SVM] Accuracy: %.2f' % accuracy_score(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #4 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Decision Tree] Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='entropy', max_depth=10, random_state=0)\n",
    "dt.fit(X_train_std, y_train)\n",
    "y_pred = dt.predict(X_test_std)\n",
    "print('[Decision Tree] Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifer #5 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Random Forest] Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf =RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=1, n_jobs=2)\n",
    "rf.fit(X_train_std, y_train)\n",
    "y_pred = rf.predict(X_test_std)\n",
    "print('[Random Forest] Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #6 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNN] Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5, p = 2, metric = 'minkowski')\n",
    "knn.fit(X_train_std, y_train)\n",
    "y_pred = knn.predict(X_test_std)\n",
    "print('[KNN] Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #7 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Naive Bayes] Accuracy: 0.77\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_std, y_train)\n",
    "y_pred = gnb.predict(X_test_std)\n",
    "print('[Naive Bayes] Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Best classifers: *Linear SVM*, *Random Forest*, *KNN*\n",
    "\n",
    "* KNN: deterimines the class label of a new data point by a majority vote among its k nearest neighbours. The cost of the learning process is zero. \n",
    "* Random Forest: ensembles of decision tree in some way. Weak learners are combined to build a more robust model (strong learner) that has better generalization error and is less susceptible to overfitting. \n",
    "* Linear SVM: maximizes margins to nearest samples and is more robust against outliers.\n",
    "\n",
    "Worst classifer: *Naive Bayes* (with accuracy score 0.77), because\n",
    "\n",
    "* Its strong feature independence assumption may not help in real practices. \n",
    "* Specifying a prior is theoratically possible but infeasible in many cases. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
